Pattern matching is to find a pattern, which is relatively small, in a text, which is supposed to be very large.
Patterns and texts can be one-dimensional, or two dimensional.
In the case of one-dimensional, examples can be text editor and DNA analysis.
In the text editor, we have 26 characters and some special symbols, whereas in the DNA case, we have four characters of A, C, G, and T.
In the text editor, a pattern is often a word, whose length is around 10, and the length of the text is a few hundred up to one million.
In the DNA analysis, a gene in a few hundred long and the human genome is about 3 billion long.
In the case of two-dimensional, the typical application is a pattern matching in computer vision.
A pattern is about (100, 100) and text typically (1024,1024).
Either one-dimensional or two-dimensional, the text is very large, and therefore a fast algorithm to find the occurrence(s) of pattern in it is needed.
We start from a naive algorithm for one-dimensional.
At first the pattern is set to the left end of the text, and matching process starts.
After a mismatch is found, pattern is shifted one place right and a new matching process starts, and so on.
The pattern and text are in arrays pat[1m] and text[1n] respectively.
Although there are a few thousand variations of the computer with different archi-tectures and internal organization, it is best to think about them at the level of the assembly language.
Despite architectural variations, the assembly level language sup- port is very similar - the major difference being in the number of registers and the word length of the machine.
But these parameters are also in a restricted range of a factor of two, and hence asymptotically in the same ball park.
In summary, think about any computer as a machine that supports a basic instruction set consisting of arithmetic and logical operations and memory accesses (including indirect addressing).
We will avoid cumbersome details of the exact instruction set and assume realistically that any instruction of one machine can be simulated using a constant number of available instruction of another machine.
Since analysis of algorithms involves counting the number of operations and not the exact timings (which could differ by an order of magnitude), the above simplification is justified.
The careful reader would have noticed that during our detailed analysis of Method 3 in the previous sections, we were not simply counting the number of arithmetic operations but actually the number of bit-level operations.
Therefore the cost of a multiplication or addition was not unity but proportional to the length of the input.
Had we only counted the number of multiplications for computing xn, that would only be O(log n).
This would indeed be the analysis in a uniform cost model where only the number of arithmetic (also logical) operations are counted and does not depend on the length of the operands.
A very common us of this model is for comparison-based problems like sorting, selection, merging, and many data-structure operations.
For these problems, we often count only the number of comparisons (not even other arithmetic operations) without bothering about the length of the operands involved.
In other words, we implicitly assume O(1) cost for any comparison.
This is not considered unreasonable since the size of the numbers involved in sorting do not increase during the course of the algorithm for majority of the commonly known sorting problems.
On the other hand consider the following problem of repeated squaring n times starting with 2.
The resultant is a number 22n which requires 2n bits to be represented.
It will be very unreasonable to assume that a number that is exponentially long can be written out (or even stored) in O(n) time.
Therefore the uniform cost model will not reflect any realistic setting for this problem.
On the other extreme is the logarithmic cost model where the cost of an operation is proportional to length of the operands.
This is very consistent with the physical world and also has close relation with the Turing Machine model which is a favorite of complexity theorists.
Our analysis in the previous sections is actually done with this model in mind.
It is not only the arithmetic operations but also the cost of memory access is proportional to the length of the address and the operand.
The most commonly used model is something in between.
We assume that for an input of size n, any operation involving operands of size log n 2 takes O(1) steps.
This is justified as follows.
All microprocessor chips have specialized hardware circuits for arithmetic operations like multiplication, addition, division etc. that take a fixed number of clock cycles when the operands fit into a word.
The reason that log n is a natural choice for a word is that, even to address an input size n, you require log n bits of address space.
The present high end microprocessor chips have typically 2-4 GBytes of RAM and about 64 bits word size - clearly 264 exceeds 4 GBytes.
We will also use this model, popularly known as Random Access Machine (or RAM in short) except for problems that deal with numbers as inputs like multiplication in the previous section where we will invoke the log cost model.
In the beginning, it is desirable that for any algorithm, you get an estimate of the maximum size of the numbers to ensure that operands do not exceed (log n) so that it is safe to use the RAM model.
There is clear trade-off between the simplicity and the fidelity achieved by an abstract model.
One of the obvious (and sometimes serious) drawback of the RAM model is the assumption of unbounded number of registers since the memory access cost is uniform.
In reality, there is a memory hierarchy comprising of registers, several levels of cache, main memory and finally the disks.
We incur a higher access cost as we go from registers towards the disk and for techological reason, the size of the faster memory is limited.
There could be a disparity of 105 between the fastest and the slowest memory which makes the RAM model somewhat suspect for larger input sizes.
Pattern matching is to find a pattern, which is relatively small, in a text, which is supposed to be very large.
Patterns and texts can be one-dimensional, or two dimensional.
In the case of one-dimensional, examples can be text editor and DNA analysis.
In the text editor, we have 26 characters and some special symbols, whereas in the DNA case, we have four characters of A, C, G, and T.
In the text editor, a pattern is often a word, whose length is around 10, and the length of the text is a few hundred up to one million.
In the DNA analysis, a gene in a few hundred long and the human genome is about 3 billion long.
In the case of two-dimensional, the typical application is a pattern matching in computer vision.
A pattern is about (100, 100) and text typically (1024,1024).
Either one-dimensional or two-dimensional, the text is very large, and therefore a fast algorithm to find the occurrence(s) of pattern in it is needed.
We start from a naive algorithm for one-dimensional.
At first the pattern is set to the left end of the text, and matching process starts.
After a mismatch is found, pattern is shifted one place right and a new matching process starts, and so on.
The pattern and text are in arrays pat[1m] and text[1n] respectively.
Although there are a few thousand variations of the computer with different archi-tectures and internal organization, it is best to think about them at the level of the assembly language.
Despite architectural variations, the assembly level language sup- port is very similar - the major difference being in the number of registers and the word length of the machine.
But these parameters are also in a restricted range of a factor of two, and hence asymptotically in the same ball park.
In summary, think about any computer as a machine that supports a basic instruction set consisting of arithmetic and logical operations and memory accesses (including indirect addressing).
We will avoid cumbersome details of the exact instruction set and assume realistically that any instruction of one machine can be simulated using a constant number of available instruction of another machine.
Since analysis of algorithms involves counting the number of operations and not the exact timings (which could differ by an order of magnitude), the above simplification is justified.
The careful reader would have noticed that during our detailed analysis of Method 3 in the previous sections, we were not simply counting the number of arithmetic operations but actually the number of bit-level operations.
Therefore the cost of a multiplication or addition was not unity but proportional to the length of the input.
Had we only counted the number of multiplications for computing xn, that would only be O(log n).
This would indeed be the analysis in a uniform cost model where only the number of arithmetic (also logical) operations are counted and does not depend on the length of the operands.
A very common us of this model is for comparison-based problems like sorting, selection, merging, and many data-structure operations.
For these problems, we often count only the number of comparisons (not even other arithmetic operations) without bothering about the length of the operands involved.
In other words, we implicitly assume O(1) cost for any comparison.
This is not considered unreasonable since the size of the numbers involved in sorting do not increase during the course of the algorithm for majority of the commonly known sorting problems.
On the other hand consider the following problem of repeated squaring n times starting with 2.
The resultant is a number 22n which requires 2n bits to be represented.
It will be very unreasonable to assume that a number that is exponentially long can be written out (or even stored) in O(n) time.
Pattern matching is to find a pattern, which is relatively small, in a text, which is supposed to be very large.
Patterns and texts can be one-dimensional, or two dimensional.
In the case of one-dimensional, examples can be text editor and DNA analysis.
In the text editor, we have 26 characters and some special symbols, whereas in the DNA case, we have four characters of A, C, G, and T.
In the text editor, a pattern is often a word, whose length is around 10, and the length of the text is a few hundred up to one million.
In the DNA analysis, a gene in a few hundred long and the human genome is about 3 billion long.
In the case of two-dimensional, the typical application is a pattern matching in computer vision.
A pattern is about (100, 100) and text typically (1024,1024).
Either one-dimensional or two-dimensional, the text is very large, and therefore a fast algorithm to find the occurrence(s) of pattern in it is needed.
We start from a naive algorithm for one-dimensional.
At first the pattern is set to the left end of the text, and matching process starts.
After a mismatch is found, pattern is shifted one place right and a new matching process starts, and so on.
The pattern and text are in arrays pat[1m] and text[1n] respectively.
Although there are a few thousand variations of the computer with different archi-tectures and internal organization, it is best to think about them at the level of the assembly language.
Despite architectural variations, the assembly level language sup- port is very similar - the major difference being in the number of registers and the word length of the machine.
But these parameters are also in a restricted range of a factor of two, and hence asymptotically in the same ball park.
In summary, think about any computer as a machine that supports a basic instruction set consisting of arithmetic and logical operations and memory accesses (including indirect addressing).
We will avoid cumbersome details of the exact instruction set and assume realistically that any instruction of one machine can be simulated using a constant number of available instruction of another machine.
Since analysis of algorithms involves counting the number of operations and not the exact timings (which could differ by an order of magnitude), the above simplification is justified.
The careful reader would have noticed that during our detailed analysis of Method 3 in the previous sections, we were not simply counting the number of arithmetic operations but actually the number of bit-level operations.
Therefore the cost of a multiplication or addition was not unity but proportional to the length of the input.
Had we only counted the number of multiplications for computing xn, that would only be O(log n).
This would indeed be the analysis in a uniform cost model where only the number of arithmetic (also logical) operations are counted and does not depend on the length of the operands.
A very common us of this model is for comparison-based problems like sorting, selection, merging, and many data-structure operations.
For these problems, we often count only the number of comparisons (not even other arithmetic operations) without bothering about the length of the operands involved.
Design and Analysis is a beautiful subject.
In other words, we implicitly assume O(1) cost for any comparison.
This is not considered unreasonable since the size of the numbers involved in sorting do not increase during the course of the algorithm for majority of the commonly known sorting problems.
On the other hand consider the following problem of repeated squaring n times starting with 2.
The resultant is a number 22n which requires 2n bits to be represented.
It will be very unreasonable to assume that a number that is exponentially long can be written out (or even stored) in O(n) time.
This has been redressed by the external memory model.
beautiful
