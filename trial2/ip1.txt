Pattern matching is to find a pattern, which is relatively small, in a text, which is supposed to be very large. Patterns and texts can be one-dimensional, or two dimensional. In the case of one-dimensional, examples can be text editor and DNA analysis. In the text editor, we have 26 characters and some special symbols, whereas in the DNA case, we have four characters of A, C, G, and T. In the text editor, a pattern is often a word, whose length is around 10, and the length of the text is a few hundred up to one million. In the DNA analysis, a gene in a few hundred long and the human genome is about 3 billion long. In the case of two-dimensional, the typical application is a pattern matching in computer vision. A pattern is about (100, 100) and text typically (1024,1024). Either one-dimensional or two-dimensional, the text is very large, and therefore a fast algorithm to find the occurrence(s) of pattern in it is needed. We start from a naive algorithm for one-dimensional. At first the pattern is set to the left end of the text, and matching process starts. After a mismatch is found, pattern is shifted one place right and a new matching process starts, and so on. The pattern and text are in arrays pat[1..m] and text[1..n] respectively. Although there are a few thousand variations of the computer with different archi-tectures and internal organization, it is best to think about them at the level of the assembly language. Despite architectural variations, the assembly level language sup- port is very similar - the major difference being in the number of registers and the word length of the machine. But these parameters are also in a restricted range of a factor of two, and hence asymptotically in the same ball park. In summary, think about any computer as a machine that supports a basic instruction set consisting of arithmetic and logical operations and memory accesses (including indirect addressing). We will avoid cumbersome details of the exact instruction set and assume realistically that any instruction of one machine can be simulated using a constant number of available instruction of another machine. Since analysis of algorithms involves counting the number of operations and not the exact timings (which could differ by an order of magnitude), the above simplification is justified. The careful reader would have noticed that during our detailed analysis of Method 3 in the previous sections, we were not simply counting the number of arithmetic operations but actually the number of bit-level operations. Therefore the cost of a multiplication or addition was not unity but proportional to the length of the input. Had we only counted the number of multiplications for computing xn, that would only be O(log n). This would indeed be the analysis in a uniform cost model where only the number of arithmetic (also logical) operations are counted and does not depend on the length of the operands. A very common us of this model is for comparison-based problems like sorting, selection, merging, and many data-structure operations. For these problems, we often count only the number of comparisons (not even other arithmetic operations) without bothering about the length of the operands involved. In other words, we implicitly assume O(1) cost for any comparison. This is not considered unreasonable since the size of the numbers involved in sorting do not increase during the course of the algorithm for majority of the commonly known sorting problems. On the other hand consider the following problem of repeated squaring n times starting with 2. The resultant is a number 22n which requires 2n bits to be represented. It will be very unreasonable to assume that a number that is exponentially long can be written out (or even stored) in O(n) time. Therefore the uniform cost model will not reflect any realistic setting for this problem. On the other extreme is the logarithmic cost model where the cost of an operation is proportional to length of the operands. This is very consistent with the physical world and also has close relation with the Turing Machine model which is a favorite of complexity theorists. Our analysis in the previous sections is actually done with this model in mind. It is not only the arithmetic operations but also the cost of memory access is proportional to the length of the address and the operand. The most commonly used model is something in between. We assume that for an input of size n, any operation involving operands of size log n 2 takes O(1) steps. This is justified as follows. All microprocessor chips have specialized hardware circuits for arithmetic operations like multiplication, addition, division etc. that take a fixed number of clock cycles when the operands fit into a word. The reason that log n is a natural choice for a word is that, even to address an input size n, you require log n bits of address space. The present high end microprocessor chips have typically 2-4 GBytes of RAM and about 64 bits word size - clearly 264 exceeds 4 GBytes. We will also use this model, popularly known as Random Access Machine (or RAM in short) except for problems that deal with numbers as inputs like multiplication in the previous section where we will invoke the log cost model. In the beginning, it is desirable that for any algorithm, you get an estimate of the maximum size of the numbers to ensure that operands do not exceed (log n) so that it is safe to use the RAM model.There is clear trade-off between the simplicity and the fidelity achieved by an abstract model. One of the obvious (and sometimes serious) drawback of the RAM model is the assumption of unbounded number of registers since the memory access cost is uniform. In reality, there is a memory hierarchy comprising of registers, several levels of cache, main memory and finally the disks. We incur a higher access cost as we go from registers towards the disk and for techological reason, the size of the faster memory is limited. There could be a disparity of 105 between the fastest and the slowest memory which makes the RAM model somewhat suspect for larger input sizes. This has been redressed by the external memory model. We are given a set of jobs J1,J2 ...Jn, their corresponding deadlines di for completion and the corresponding penalties pi if a job completes after deadlines. The jobs have unit processing time on a single available machine. We want to minimize the total penalty incurred by the jobs that are not completed before their deadlines. Stated otherwise, we want to maximize the penalty of the jobs that get completed before their deadlines. A set A of jobs is independent if there exists a schedule to complete all jobs in A without incurring any penalty. We will try to verify Property 2. Let A,B be two independent sets of jobs with |B| > |A|. We would like to show that for some job J ∈ B, {J}∪ A is independent. Let |A| = m < n = |B|. Start with any feasible schedules for A and B and compress them, i.e. remove any idle time between the jobs by transforming the schedule where there is no gap between the ﬁnish time of a job and start time of the next. This shifting to left does not aﬀect independence. Let us denote the (ordered) jobs in A by A1,A2 ...Am and the times for scheduling of jobs in A be d1,d2 ...dm respectively. Likewise, let the jobs in B be B1,B2 ...Bn and their scheduling times d. en move Aj to the same position as Bn (this doesn’t violate its deadline) creating a gap at the j-th position in A. We can now shift-to-left the jobs in A−Aj and now by ignoring the jobs Bn = Aj, we have one less job in A and B. We can renumber the jobs and are in a similar position as before. By applying this strategy inductively, either we succeed in adding a job from B to A without conﬂict or we are in a situation where A is empty and B is not so that we can now add without conﬂict. Nevertheless let us persist with greedy, and analyse what we can achieve. Note that the edges are chosen in decreasing order of weights such that the end-points have not been chosen previously. Let us deﬁne the subset of edges chosen by greedy as G and let us denote the optimal solution by O. Clarly w(O) ≥ w(G). Consider an edge (x,y)∈ O−G. Clearly, by the time the turn of (x,y) came, some edge(s) were already chosen by greedy that was incident on either x or y. Let e′ be such an edge and clearly w(e′) ≥ w(x,y) since it was chosen earlier. If there were two separate edges that were incident on x and y chosen earlier, let e′ have the larger weight. We can therefore claim the following For every edge (x,y) ∈ O − G, there is an edge, e(x,y) ∈ G− O whose weight is greater than w(x,y). Suppose e(x,y) = (x,u) where u 6= y. Note that there may be another edge (v,u)∈ O−G which could not be chosen by greedy because of e(x,y). In summary, an edge e ∈ G−O can block atmost two edges in O to be chosen and its weight is greater than or equal to both the blocked edges. This implies that the total weight of the edges in G is at least half that of the optimal weight. So greedy in this case has some guarantee about the solution even though it is not optimal.
optimal
